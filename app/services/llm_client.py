# app/services/llm_client.py - ã‚·ãƒ³ãƒ—ãƒ«ãªLLMåˆ‡ã‚Šæ›¿ãˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
import os
import json
import httpx
from typing import Dict, Any, Optional
import random

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

class LLMClient:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªLLMåˆ‡ã‚Šæ›¿ãˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    
    ç’°å¢ƒå¤‰æ•°LLM_TYPEã§ä»¥ä¸‹ã‚’åˆ‡ã‚Šæ›¿ãˆ:
    - runpod: RunPod Llama API
    - openai: OpenAI API
    - mock: ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    
    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°ã‚’æ˜ç¤ºçš„ã«å†èª­ã¿è¾¼ã¿
        from dotenv import load_dotenv
        load_dotenv(override=True)
        
        self.llm_type = os.getenv("LLM_TYPE", "mock").lower()
        
        # RunPodè¨­å®š
        self.runpod_url = os.getenv("LLAMA_API_URL", "http://localhost:8080")
        self.runpod_key = os.getenv("LLAMA_API_KEY", "")
        
        # OpenAIè¨­å®š
        self.openai_key = os.getenv("OPENAI_API_KEY", "")
        if OPENAI_AVAILABLE and self.openai_key:
            self.openai_client = openai.OpenAI(api_key=self.openai_key)
        else:
            self.openai_client = None
        
        print(f"LLMClient initialized with provider: {self.llm_type}")
    
    async def generate(self, prompt: str, max_tokens: int = 500) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            if self.llm_type == "runpod":
                return await self._runpod_generate(prompt, max_tokens)
            elif self.llm_type == "openai":
                return await self._openai_generate(prompt, max_tokens)
            else:
                return self._mock_generate(prompt)
        except Exception as e:
            print(f"LLM generation failed ({self.llm_type}): {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ãƒ¢ãƒƒã‚¯ã‚’è¿”ã™
            return self._mock_generate(prompt)
    
    async def _runpod_generate(self, prompt: str, max_tokens: int) -> str:
        """RunPod Llama APIã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.runpod_url or not self.runpod_key:
            raise ValueError("RunPod configuration missing")
        
        # RunPod Ollama APIå½¢å¼
        payload = {
            "model": "llama2",  # ã¾ãŸã¯åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«å
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": max_tokens,
                "temperature": 0.7
            }
        }
        
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": self.runpod_key
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{self.runpod_url}/ollama/api/generate",
                json=payload,
                headers=headers
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "").strip()
    
    async def _openai_generate(self, prompt: str, max_tokens: int) -> str:
        """OpenAI APIã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.openai_client:
            raise ValueError("OpenAI client not available")
        
        # OpenAI APIã¯åŒæœŸé–¢æ•°ãªã®ã§ã€awaitã¯ä¸è¦
        response = self.openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
    
    def _mock_generate(self, prompt: str) -> str:
        """ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        
        # Chronicle Ambient Pulseç”¨ã®JSONå¿œç­”
        if "ç’°å¢ƒéŸ³" in prompt and "JSONå½¢å¼" in prompt:
            mock_ambient_responses = [
                {
                    "emotion": {
                        "primary": "âœ¨ã‚ãã‚ã",
                        "intensity": 4,
                        "color": "#ff6b6b",
                        "pulse_speed": "medium",
                        "texture": "sparkling"
                    },
                    "atmosphere": {
                        "description": "æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã®èŠ½ãŒè‚²ã¤ã€å‰µé€ çš„ãªã‚¨ãƒãƒ«ã‚®ãƒ¼ã«æº€ã¡ãŸç©ºé–“ã§ã™",
                        "mood_emoji": "âœ¨ğŸ’¡ğŸŒŸ",
                        "energy_level": 8
                    },
                    "story_potential": {
                        "score": 8,
                        "moment_type": "å‰µé€ ã®ç¬é–“",
                        "capture_worthy": True,
                        "suggested_title": "ã‚¢ã‚¤ãƒ‡ã‚¢ãŒèŠ±é–‹ã„ãŸåˆå¾Œ"
                    }
                },
                {
                    "emotion": {
                        "primary": "ğŸ’•ã»ã‚“ã‚ã‹",
                        "intensity": 3,
                        "color": "#4ecdc4",
                        "pulse_speed": "gentle",
                        "texture": "warm"
                    },
                    "atmosphere": {
                        "description": "ç©ã‚„ã‹ãªåˆå¾Œã®å…‰ã«åŒ…ã¾ã‚Œã¦ã€å¿ƒåœ°ã‚ˆã„ä¼šè©±ãŒæµã‚Œã¦ã„ã¾ã™",
                        "mood_emoji": "ğŸ˜Šâ˜•ğŸŒ…",
                        "energy_level": 6
                    },
                    "story_potential": {
                        "score": 7,
                        "moment_type": "æ—¥å¸¸ã®è¼ã",
                        "capture_worthy": True,
                        "suggested_title": "ã‚«ãƒ•ã‚§ã«å’²ã„ãŸå°ã•ãªèŠ±"
                    }
                },
                {
                    "emotion": {
                        "primary": "ğŸ¤”æ·±ã„è©±",
                        "intensity": 4,
                        "color": "#45b7d1",
                        "pulse_speed": "slow",
                        "texture": "deep"
                    },
                    "atmosphere": {
                        "description": "å¿ƒã®å¥¥æ·±ãã«éŸ¿ãã€å¤§åˆ‡ãªæƒ³ã„ãŒäº¤ã‚ã•ã‚Œã‚‹ç‰¹åˆ¥ãªæ™‚é–“ã§ã™",
                        "mood_emoji": "ğŸ¤”ğŸ’­ğŸ’«",
                        "energy_level": 7
                    },
                    "story_potential": {
                        "score": 9,
                        "moment_type": "å¿ƒã®äº¤æµ",
                        "capture_worthy": True,
                        "suggested_title": "ã¤ãªãŒã‚Šã‚’æ„Ÿã˜ãŸç¬é–“"
                    }
                }
            ]
            
            # ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã—ã¦JSONæ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™
            selected_response = random.choice(mock_ambient_responses)
            return json.dumps(selected_response, ensure_ascii=False, indent=2)
        
        # å¾“æ¥ã®ãƒ¢ãƒƒã‚¯å¿œç­”
        mock_responses = [
            "ã“ã‚Œã¯æ„Ÿæƒ…è±Šã‹ãªä¼šè©±ã§ã™ã­ã€‚æ¸©ã‹ã„é›°å›²æ°—ãŒä¼ã‚ã£ã¦ãã¾ã™ã€‚",
            "ã¨ã¦ã‚‚èˆˆå‘³æ·±ã„å†…å®¹ã§ã™ã€‚å‚åŠ è€…ã®çš†ã•ã‚“ã®å€‹æ€§ãŒå…‰ã£ã¦ã„ã¾ã™ã€‚",
            "å¿ƒæ¸©ã¾ã‚‹äº¤æµãŒæ„Ÿã˜ã‚‰ã‚Œã‚‹ç´ æ•µãªä¼šè©±ã§ã™ã€‚",
            "ç™ºè¦‹ã¨å­¦ã³ã«æº€ã¡ãŸæœ‰æ„ç¾©ãªæ™‚é–“ã ã£ãŸã‚ˆã†ã§ã™ã­ã€‚",
            "ã¿ã‚“ãªã§éã”ã—ãŸç‰¹åˆ¥ãªç¬é–“ãŒç¾ã—ãæã‹ã‚Œã¦ã„ã¾ã™ã€‚"
        ]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã«å¿œã˜ã¦é©åˆ‡ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é¸æŠ
        if "æ„Ÿæƒ…" in prompt or "emotion" in prompt.lower():
            return "ã“ã®ä¼šè©±ã‹ã‚‰ã¯æ¸©ã‹ã„æ„Ÿæƒ…ã¨è¦ªã—ã¿ã‚„ã™ã„é›°å›²æ°—ãŒæ„Ÿã˜ã‚‰ã‚Œã¾ã™ã€‚å‚åŠ è€…åŒå£«ã®è‰¯å¥½ãªé–¢ä¿‚æ€§ãŒä¼ã‚ã£ã¦ãã¾ã™ã€‚"
        elif "ã‚¿ã‚¤ãƒˆãƒ«" in prompt or "title" in prompt.lower():
            titles = [
                "ã‚«ãƒ•ã‚§ã§ç”Ÿã¾ã‚ŒãŸå°ã•ãªç™ºè¦‹",
                "å¿ƒæ¸©ã¾ã‚‹åˆå¾Œã®ä¼šè©±",
                "æ–°ã—ã„ä»²é–“ã¨ã®ç‰¹åˆ¥ãªæ™‚é–“",
                "ã¿ã‚“ãªã§ç´¡ã„ã ç¾ã—ã„ç‰©èª",
                "ã„ã¤ã‚‚ã®ã‚«ãƒ•ã‚§ã§ã®ç´ æ•µãªå‡ºä¼šã„"
            ]
            return random.choice(titles)
        elif "æ”¹å–„" in prompt or "enhance" in prompt.lower():
            return "ãã‚Œã¯ã€ã„ã¤ã‚‚ã®ã‚«ãƒ•ã‚§ã§ã®ã“ã¨ã§ã—ãŸã€‚\n\nåˆå¾Œã®æŸ”ã‚‰ã‹ãªé™½å°„ã—ãŒçª“ã‹ã‚‰å·®ã—è¾¼ã‚€ä¸­ã€æ–°ã—ã„å‡ºä¼šã„ãŒç”Ÿã¾ã‚Œã‚ˆã†ã¨ã—ã¦ã„ã¾ã—ãŸã€‚æœ€åˆã¯å°‘ã—ç·Šå¼µæ°—å‘³ã ã£ãŸä¼šè©±ã‚‚ã€å…±é€šã®è©±é¡ŒãŒè¦‹ã¤ã‹ã‚‹ã¨ã€ã ã‚“ã ã‚“ã¨å’Œã‚„ã‹ãªé›°å›²æ°—ã«å¤‰ã‚ã£ã¦ã„ãã¾ã™ã€‚\n\nãŠäº’ã„ã®ç¬‘é¡”ãŒäº¤ã‚ã•ã‚Œã‚‹ç¬é–“ã€ãã“ã«ã¯ç‰¹åˆ¥ãªæ¸©ã‹ã•ãŒç”Ÿã¾ã‚Œã¦ã„ã¾ã—ãŸã€‚ä½•æ°—ãªã„ä¼šè©±ã®ä¸­ã«ã‚‚ã€ãã£ã¨å¤§åˆ‡ãªä½•ã‹ãŒéš ã•ã‚Œã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ã€‚\n\nãã‚“ãªå°ã•ãªç™ºè¦‹ãŒã€ä»Šæ—¥ã‚‚ã‚«ãƒ•ã‚§ã®ç‰‡éš…ã§é™ã‹ã«è¼ã„ã¦ã„ã¾ã™ã€‚"
        else:
            return random.choice(mock_responses)
    
    def get_provider_info(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’å–å¾—"""
        return {
            "provider": self.llm_type,
            "runpod_configured": bool(self.runpod_url and self.runpod_key),
            "openai_configured": bool(self.openai_client),
            "available_providers": ["runpod", "openai", "mock"]
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
llm_client = LLMClient()
